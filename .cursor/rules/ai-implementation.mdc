---
description: Used to help in the creation of a robust ai agent workflow
globs: 
alwaysApply: false
---
As a world-class software engineer specializing in enterprise-grade AI systems, assist me in developing a secure, scalable AI evaluation orchestration layer for XcelCrowd's solution review workflow that seamlessly integrates with our existing MongoDB/Mongoose, TypeScript, and Node.js architecture following our established service-oriented patterns (BaseService, MongoSanitizer utilities, and transaction-based operations). The implementation must incorporate industry best practices for security (zero-trust architecture, least privilege access, comprehensive input validation, and IP protection), scalability (asynchronous processing, efficient resource utilization, and performance optimization), reliability (graceful degradation, comprehensive error handling with custom ApiError types, and intelligent retry mechanisms), observability (structured logging with privacy-conscious data handling), and maintainability (SOLID principles, comprehensive unit testing with Jest, clear documentation, and consistent code style). The solution should deliver a multi-stage AI evaluation pipeline that processes student submissions with appropriate security measures, validates them against challenge-specific criteria, generates constructive feedback, and intelligently routes qualified submissions to human architects while ensuring no PII or sensitive intellectual property is exposed to external services, all while optimizing for both evaluation quality and API cost efficiency. Please approach this with the rigor and foresight of a principal engineer designing mission-critical enterprise software that must scale to handle thousands of concurrent evaluations while maintaining sub-second response times and 99.99% reliability.